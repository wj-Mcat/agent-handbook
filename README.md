<h1 align="center">
	<p align="center">
        ğŸ”® Awesome AI Agents
		<a href="https://x.com/wj_Mcat" target="_blank">
			<img src="https://img.shields.io/twitter/follow/wj_Mcat.svg?logo=twitter">
		</a>
	</p>
</h1>

## å› ä¸ºçƒ­çˆ±ï¼Œæ‰€ä»¥åˆ†äº«

æˆ‘å§‹ç»ˆç›¸ä¿¡å¼€æºï¼Œä¹Ÿçƒ­çˆ±åˆ†äº«åœ¨å·¥ä½œå­¦ä¹ è¿‡ç¨‹ä¸­å¯¹äºAgentæ‰€æœ‰è§‰å¾—æœ‰ä»·å€¼ã€æœ‰æ„æ€çš„çŸ¥è¯†ç‚¹ï¼Œå¹¶å®šæœŸå°†å…¶ç¼–å†™æˆä¸€ç¯‡ç¯‡åšå®¢ï¼Œè¿›è€Œè·Ÿå¤§å®¶è®¨è®ºå­¦ä¹ ï¼Œå…±åŒè¿›æ­¥ã€‚

ä¹Ÿéå¸¸æ¬¢è¿å¤§å®¶èƒ½å¤Ÿä¸€èµ·è´¡çŒ® PR æ¥ä¸æ–­å®Œå–„æ­¤åšå®¢ï¼Œä¸æ–­å®Œå–„ï¼Œè®©å…¶æˆä¸ºä¸€ä¸ªçœŸæ­£çš„ Agent Handbookã€‚

## çŸ¥è¯†ç‚¹

### Agent Introduction

[![What's next for AI agentic workflows ft. Andrew Ng of AI Fund](https://img.youtube.com/vi/sal78ACtGTc/0.jpg)](https://www.youtube.com/watch?v=sal78ACtGTc)

### Agent Workflow

## Paper Reading

### ORPO: Monolithic Preference Optimization without Reference Model

ORPO æå‡ºäº†ä¸€ä¸ªéå¸¸åˆ›æ–°çš„æ–¹æ³•ï¼šå°† æ¨¡å‹å¯¹é½é˜¶æ®µ å’Œ SFTé˜¶æ®µ èåˆåˆ°ä¸€èµ·ï¼Œè¿›è€Œæå‡æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ã€‚

åœ¨ SFT é˜¶æ®µï¼Œå°±ç›´æ¥å°†å¯¹é½çš„æ•°æ®åŠ å…¥åˆ°è®­ç»ƒå½“ä¸­ï¼Œè¿›è€Œåœ¨SFT é˜¶æ®µå°±å·²ç»å®ç°äº†æ¨¡å‹å¯¹é½çš„èƒ½åŠ›ã€‚

### Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models

`è§£å†³çš„é—®é¢˜`ï¼šæ­¤è®ºæ–‡æ—¨åœ¨æå‡æä¾›ä¸€ä¸ªåˆ›å»ºé«˜è´¨é‡æŒ‡ä»¤è·Ÿéšæ•°æ®é›†çš„æ–¹æ³•ï¼Œè¿›è€Œæé«˜åœ¨ä¸åŒæ–¹æ³•ä¸­æŒ‡ä»¤å­¦ä¹ çš„èƒ½åŠ›ã€‚

æ­¤è®ºæ–‡ä¸­æ˜¯é€šè¿‡ç”Ÿæˆä¸€ä¸ªå‡½æ•°å‡½æ•°æ¥æ£€æµ‹ Response å†…å®¹æ˜¯å¦æ­£ç¡®ï¼Œè¿›è€Œæå‡æ•°æ®è´¨é‡ã€‚

> æ­¤è®ºæ–‡çš„æ–¹æ³•ä¸ç®—æ˜¯å¾ˆåˆ›æ–°ï¼Œå¯æ˜¯ä»ä¸€å®šç¨‹åº¦ä¸Šå‘Šè¯‰æˆ‘ä»¬ï¼šæ•°æ®è´¨é‡çš„é‡è¦æ€§ã€‚

## Join the community

- Follow us on [X ](https://twitter.com/wj_Mcat)
- [Hit us up on discord](https://discord.gg/gJNKfdTr)
- Get my latest blogs on [çŸ¥è¯†æ˜Ÿçƒ](https://t.zsxq.com/soEav)

