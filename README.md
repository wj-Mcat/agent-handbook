<h1 align="center">
	<p align="center">
        ğŸ”® Awesome AI Agents
		<a href="https://x.com/wj_Mcat" target="_blank">
			<img src="https://img.shields.io/twitter/follow/wj_Mcat.svg?logo=twitter">
		</a>
	</p>
</h1>

## åˆè¡·

åˆ†äº«è‡ªå·±åœ¨å·¥ä½œå­¦ä¹ è¿‡ç¨‹ä¸­å¯¹äºAgentæ‰€æœ‰çš„çŸ¥è¯†ç‚¹ï¼Œå¹¶å®šæœŸå°†å…¶ç¼–å†™æˆä¸€ç¯‡ç¯‡åšå®¢ï¼Œè¿›è€Œè·Ÿå¤§å®¶è®¨è®ºå­¦ä¹ ï¼Œå…±åŒè¿›æ­¥ã€‚

## Paper Reading

* **ORPO: Monolithic Preference Optimization without Reference Model**

ORPO æå‡ºäº†ä¸€ä¸ªéå¸¸åˆ›æ–°çš„æ–¹æ³•ï¼šå°† æ¨¡å‹å¯¹é½é˜¶æ®µ å’Œ SFTé˜¶æ®µ èåˆåˆ°ä¸€èµ·ï¼Œè¿›è€Œæå‡æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ã€‚

åœ¨ SFT é˜¶æ®µï¼Œå°±ç›´æ¥å°†å¯¹é½çš„æ•°æ®åŠ å…¥åˆ°è®­ç»ƒå½“ä¸­ï¼Œè¿›è€Œåœ¨SFT é˜¶æ®µå°±å·²ç»å®ç°äº†æ¨¡å‹å¯¹é½çš„èƒ½åŠ›ã€‚

* **Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models**

`è§£å†³çš„é—®é¢˜`ï¼šæ­¤è®ºæ–‡æ—¨åœ¨æå‡æä¾›ä¸€ä¸ªåˆ›å»ºé«˜è´¨é‡æŒ‡ä»¤è·Ÿéšæ•°æ®é›†çš„æ–¹æ³•ï¼Œè¿›è€Œæé«˜åœ¨ä¸åŒæ–¹æ³•ä¸­æŒ‡ä»¤å­¦ä¹ çš„èƒ½åŠ›ã€‚

æ­¤è®ºæ–‡ä¸­æ˜¯é€šè¿‡ç”Ÿæˆä¸€ä¸ªå‡½æ•°å‡½æ•°æ¥æ£€æµ‹ Response å†…å®¹æ˜¯å¦æ­£ç¡®ï¼Œè¿›è€Œæå‡æ•°æ®è´¨é‡ã€‚

> æ­¤è®ºæ–‡çš„æ–¹æ³•ä¸ç®—æ˜¯å¾ˆåˆ›æ–°ï¼Œå¯æ˜¯ä»ä¸€å®šç¨‹åº¦ä¸Šå‘Šè¯‰æˆ‘ä»¬ï¼šæ•°æ®è´¨é‡çš„é‡è¦æ€§ã€‚

## Join the community

- Follow us on [X ](https://twitter.com/wj_Mcat)
- [Hit us up on discord](https://discord.gg/gJNKfdTr)
- Get my latest blogs on [çŸ¥è¯†æ˜Ÿçƒ](https://t.zsxq.com/soEav)

